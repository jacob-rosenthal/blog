[
  {
    "objectID": "posts/2019-09-26-camera-obscura.html",
    "href": "posts/2019-09-26-camera-obscura.html",
    "title": "Building a Camera Obscura",
    "section": "",
    "text": "About\nFor this project, I worked with my partner Lucie Gillet to build a camera obscura from scratch. We worked on this as part of an assignment for MIT 6.869: Advances in Computer Vision (Fall 2019) taught by Professors Bill Freeman, Antonio Torralba, and Philip Isola. Course website here.\n\n\nWhat is a camera obscura?\nA camera obscura is a dark chamber with a single, tiny hole (the pinhole) in one side that lets light in. Rays of light from the outside enter the pinhole, and hit the wall opposite the pinhole, forming a projected image:\n\nThe image is inverted top-bottom and right-left because it is projected through the pinhole.\nThe image is projected inside the camera - so how do we see it? Some camera obscuras are built as structures large enough to go inside and look at the projection on a screen. However, in our case we created a small camera from a cardboard box and then used the digital cameras on our phones to capture the projected images.\n\n\nHow to build the pinhole camera\nInstructions copied from the assignment\n\nFind a cardboard box.\nMake a tiny hole (~= 5 mm) in the center of one of the faces. This will be your pinhole.\nMake a second hole, just big enough for your camera. If you’re using a professional camera, make a hole just wide enough for your lens to fit. If you’re using a phone, make a small hole such that you can press your phone to the box, and the camera will not be obstructed.\nCover the inside walls with black paper to reduce inter-reflections.\nPlace a sheet (or more) of white paper on the wall opposite your pinhole. This white paper will be the screen onto which your image is projected.\n(Optional) You may also want to use dark tape along the edges and corners of your box - where two separate pieces of cardboard meet, there may a small slit, which can let light in! (we ended up wrapping the whole camera in foil to solve this problem)\n\nHere’s what our camera looked like from the outside:\n\n\n\nOutside of camera, wrapped in foil to stop any light from entering\n\n\nAnd here is a side view of the inside, showing the arrangement of the pinholes and the screen:\n\n\n\nView inside the camera\n\n\n\n\nTaking Pictures\nSince a camera obscura lets in so little light (to keep the image sharp), we made sure to get as much light as possible by taking our pictures outside, in the daytime, on a sunny day.\nIn our first attempt, we learned the camera must remain perfectly still - if it moves at all, the images will be blurry:\n\n\n\nCapturing sharp images was difficult!\n\n\nAfter some trial and error, we managed to capture some good quality pictures:\n\n\n\nMIT columns\n\n\n\n\n\nStreet scene in Cambridge\n\n\n\n\nConclusion\nMaking a camera obscura is a fun afternoon project, and you can get surprisingly good results! Make sure to block out all light from entering the box through holes or cracks, and try it out on a sunny day!"
  },
  {
    "objectID": "posts/2023-05-27-zotero-latex.html",
    "href": "posts/2023-05-27-zotero-latex.html",
    "title": "Configuring Zotero and LaTeX",
    "section": "",
    "text": "About\nI started using Zotero this year for reference management. I like Zotero because it makes it easy to keep articles organized by creating hierarchical “collections” of articles. Each collection represents a different concept (e.g., a research area, a current project, or a course I’m taking) and then I add all the relevant articles to that collection, making it really easy to go through them later. It also comes with browser plug-ins that let you save articles from many websites in a single click, and a Microsoft Word plug-in that lets you add references from your library while writing. It took some initial time investment to import and organize everything, but it has definitely been worth it, and using a reference manager has been much better than my old system (i.e., downloading article pdfs into dropbox and never being able to find them again). As a bonus, I’ve found the Zotero iPad app to be pretty good for reading and annotating articles.\n\n\nThe Problem\nZotero unfortunately doesn’t have great support for writing documents in LaTeX. I wanted keyboard shortcuts to be able to quickly copy in-text citations (e.g. \\cite{articleTag}) and also the corresponding BibTeX entries. I also wanted to be able to control what fields were included in BibTeX entries - for example, I didn’t want my .bib files to be bloated by including entire abstracts for each article, which seems to be the default behavior.\n\n\nThe Solution\nI ended up finding a solution by combining the Zotero plug-ins Better BibTeX (BBT) and Zutilo. It took some time to figure out how to configure everything to get what I wanted, so I am documenting it here for future reference.\n\nInstall Better BibTeX and Zutilo plug-ins\nConfigure Better BibTeX citation preferences:\n\nUnder Tools &gt; Better BibTeX Preferences... &gt; Export &gt; Fields &gt; Fields to omit from export put abstract,file\n\nLink Zutilo and Better BibTeX:\n\nOpen advanced preferences: Zotero &gt; Settings &gt; Advanced &gt; General &gt; Config Editor\nSet extensions.zutilo.quickCopy_alt1 to export=a515a220-6fef-45ea-9842-8025dfebcc8f (which is BBT quick copy)\nSet extensions.zutilo.quickCopy_alt2 to export=ca65189f-8815-4afe-8c8b-8c7c15f0edca (which is BBT BibTeX copy)\n(these instructions are from here)\n\nSet keyboard shortcuts:\n\nOpen Tools &gt; Zutilo Preferences... &gt; Shortcuts\nSet QuickCopy items (alt 1) to Command-Shift-1 (⌘ ⇧ !)\nSet QuickCopy items (alt 2) to Command-Shift-2 (⌘ ⇧ @)\n\n\n\n\nResult\nAll done! Now, the keyboard shortcuts from Zutilo are linked to the BibTeX citations export tools from Better BibTeX and can be used to quickly copy references and paste them into LaTeX documents.\n\nCommand-Shift-1 will copy the inline citation, e.g. \\cite{articleTag2023}, which can be pasted into the .tex file.\nCommand-Shift-2 will copy the full BibTeX citation, which can be pasted into the .bib file\n\nThis also works with multiple articles selected, which is pretty neat!\n\n\nIn Conclusion\nI hope that LLMs will finally make LaTeX obsolete, sooner rather than later. In the meantime, this is my current workflow for inserting references from Zotero into LaTeX documents on Overleaf."
  },
  {
    "objectID": "posts/2020-10-15-wine-review-generation.html",
    "href": "posts/2020-10-15-wine-review-generation.html",
    "title": "The Markovian Sommelier",
    "section": "",
    "text": "Robot Sommelier\n“Rich Tannins.”\n“Peppery finish.”\n“Afternotes of loamy soil.”\nWho writes wine descriptions, anyways? Wine reviews are practically a genre of their own, with a specific vocabulary and its own set of phrases and that I basically never see in any other context.\nIn this projet we will make a very simple model that randomly generates new wine reviews. I will walk through each step in designing the model and implementing it!"
  },
  {
    "objectID": "posts/2020-10-15-wine-review-generation.html#defining-the-model",
    "href": "posts/2020-10-15-wine-review-generation.html#defining-the-model",
    "title": "The Markovian Sommelier",
    "section": "Defining the model",
    "text": "Defining the model\nThe model we will be using is a very simple Markov chain model. First, we model each wine review as a sequence of word pairs (i.e. bigrams). Then, we create new reviews by chaining together word pairs using a single rule which is used to generate the next word given the preceding word as input. We simply look through a dataset of real wine reviews and find all occurences of the preceding word, then randomly pick one of them and use whatever word followed it in that context.\nHere’s the algorithm for generating the n-th word \\(w_n\\) given the preceding word \\(w_{n-1}\\) and a dataset \\(D\\):\nAlgorithm \\(g(w_n | w_{n-1}, D)\\):\n\nFind \\(O = \\{o_1, o_2, \\dots, o_m\\}\\), the set of all \\(m\\) occurences of \\(w_{n-1}\\) in \\(D\\)\nRandomly choose an occurence \\(o_k \\in O\\)\n\nReturn the word immediately following \\(o_k\\) in its original context\n\nBecause the generation of each word depends only on the previous word, it is completely independent of all the other preceding words in the description so far. In other words, \\(P(w_n | w_{n-1}) = P(w_n | w_{n-1}, w_{n-2}, \\dots, w_{1})\\). This means that our model is a Markovian process. The transition probabilities between bigrams are empirically determined from our corpus.\nOf course this is probably not going to be a great model, since it does not consider any of the context besides the immediately preceding word. But it can still give surprisingly good results, as it lets us capture many of the common two-word phrases which define the genre of wine reviews.\nNow let’s take a look at implementing this model."
  },
  {
    "objectID": "posts/2020-10-15-wine-review-generation.html#loading-the-data",
    "href": "posts/2020-10-15-wine-review-generation.html#loading-the-data",
    "title": "The Markovian Sommelier",
    "section": "Loading the Data",
    "text": "Loading the Data\nLuckily, someone has already gone through the effort of creating a dataset of more than 280,000 real wine descriptions! These were scraped from Wine Enthusiast and the dataset is hosted on Kaggle. The data have been downloaded and placed in the ./data folder. The data are split into two files.\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter, defaultdict\nimport spacy\n\n# first load data\ndata1 = pd.read_csv('./data/winemag-data-130k-v2.csv')\ndata2 = pd.read_csv('./data/winemag-data_first150k.csv')\n\nprint(data1.shape)\nprint(data2.shape)\n\n(129971, 14)\n(150930, 11)\n\n\nLet’s take a quick look at the datasets:\n\ndata1.head(1)\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\ntaster_name\ntaster_twitter_handle\ntitle\nvariety\nwinery\n\n\n\n\n0\n0\nItaly\nAromas include tropical fruit, broom, brimston...\nVulkà Bianco\n87\nNaN\nSicily & Sardinia\nEtna\nNaN\nKerin O’Keefe\n@kerinokeefe\nNicosia 2013 Vulkà Bianco (Etna)\nWhite Blend\nNicosia\n\n\n\n\n\n\n\n\ndata2.head(1)\n\n\n\n\n\n\n\n\nUnnamed: 0\ncountry\ndescription\ndesignation\npoints\nprice\nprovince\nregion_1\nregion_2\nvariety\nwinery\n\n\n\n\n0\n0\nUS\nThis tremendous 100% varietal wine hails from ...\nMartha's Vineyard\n96\n235.0\nCalifornia\nNapa Valley\nNapa\nCabernet Sauvignon\nHeitz\n\n\n\n\n\n\n\nFor this model, we are only interested in the descriptions, so let’s pull those out and combine all the descriptions from both files:\n\ndescriptions = list(data1[\"description\"].values) + list(data2[\"description\"].values)\n\n# strip any leading or trailing whitespace if any\ndescriptions = [string.strip() for string in descriptions]\n\nprint(\"Total number of descriptions: \", len(descriptions))\n\nTotal number of descriptions:  280901\n\n\nLet’s take a look at a few examples:\n\nfor item in np.random.choice(descriptions, size = 3): print(item, \"\\n\")\n\nSweet mocha and coffee notes overwhelm the bouquet of this Pinot, with red raspberry and cherry skin notes providing support. Lively acidity and a satiny texture fill the mouth, while white pepper spice lingers on the finish. \n\nHints of nail polish and flavors of hard citrus candy, with grainy honey and sugar. This is not a shy Riesling; it's intense, rich with peach and apricot, and pushed just a bit too far for some tastes. \n\nProduced by the owners of Châteauneuf-du-Pape estate Château Mont-Redon, this is a full and fruity wine. It has a good balance between acidity and red berry fruits that give a rich character. Packed with flavor, it's ready to drink."
  },
  {
    "objectID": "posts/2020-10-15-wine-review-generation.html#preprocessing-the-data",
    "href": "posts/2020-10-15-wine-review-generation.html#preprocessing-the-data",
    "title": "The Markovian Sommelier",
    "section": "Preprocessing the Data",
    "text": "Preprocessing the Data\nNow we need to process the data to get ready for our model. But what is the best way to do this?\n\nData Structure\nFirst we need to choose the data structure we will use. At its heart, our model relies on consectutive word pairs. So we could parse our dataset into a list of all word pairs, and then generate by filtering the list and randomly choosing.\nHowever, we know that many of the word pairs will appear quite frequently! If we just parse into a list of all word pairs, we might have 100 identical entries on our list for “rich tannins.” We can instead count how times a word pair occurs, and keep track of the counts of all the tokens. When it comes time to sample the next word, we can simply use probabilities proportional to the counts instead of uniformly sampling! This will let us generate words without having to process the entire set of all the token pairs in our entire dataset.\nIn python, we will implement this as a dictionary, where each key is a token. I’ll call this our vocabulary. The corresponding values are dictionaries themselves containing counts of all the tokens that followed.\n\n\nTokenizing\nEach descriptions in the dataset is a single string. We need to divide the descriptions into their individual words, so we can count the word pairs. This process is called tokenization, where we divide the input into a set of tokens.\nRather than doing this from scratch, we will use a pre-made tokenizer from Spacy. The advantage of this is that the pre-made tokenizer is smart enough to handle things like puncuation.\n\n%%time\n\n# use pre-made tokenizer from spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# a dictionary will be used to hold the vocabulary\n# each item in the vocabulary will have a counter to track which words follow it\npair_freq = defaultdict(Counter)\n\n# make a special end of sentence token\nend_token = \"END_TOKEN\"\n\n# process all the descriptions\n# disabling unneeded components in the pipeline to speed it up\nfor description in nlp.pipe(descriptions, disable=[\"tagger\", \"parser\", \"ner\"]):\n    # for each token, update the counts of the following word\n    for token in description:\n        # get the following token\n        try:\n            neighbor = token.nbor().text\n        except IndexError:\n            neighbor = end_token\n        \n        pair_freq[token.text][neighbor] += 1\n\nvocab = list(pair_freq.keys())\nprint(\"Total number of words:\", len(vocab))\n\nTotal number of words: 45481\nCPU times: user 1min 25s, sys: 404 ms, total: 1min 26s\nWall time: 1min 27s\n\n\n\nimport json\nwith open('robosomm_data.json', 'w') as fp:\n    json.dump(pair_freq, fp)\n\n\nimport pickle\nwith open('robosomm_data.pickle', 'wb') as handle:\n    pickle.dump(pair_freq, handle)\n\nOur vocabulary consists of more than 45,000 unique words!\nLet’s look at some random examples of word pairs:\n\nfor token1 in np.random.choice(vocab, size = 10): \n    all_following = list(pair_freq[token1].keys())\n    token2 = np.random.choice(all_following)\n    print(token1, token2)\n\ndottings of\nripper ,\ncolada ,\nassemblng quite\nblackberry clusters\ngallo salsa\nBarefoot sparkling\nsections that\nCarpoli has\nsauvage wildness"
  },
  {
    "objectID": "posts/2020-10-15-wine-review-generation.html#implementing-the-model",
    "href": "posts/2020-10-15-wine-review-generation.html#implementing-the-model",
    "title": "The Markovian Sommelier",
    "section": "Implementing the model",
    "text": "Implementing the model\nFirst, we implement our function to generate the next word. Because we preprocessed the data in a smart way, this is actually very simple!\n\n# functions to generate text\ndef gen_next_word(word):\n    \"\"\"Generate the next word given the preceding word\"\"\"\n    # Get the counter for the following words\n    all_following = pair_freq[word]\n    # Get the words themselves, and corresponding counts\n    following_words = list(all_following.keys())\n    counts = np.array(list(all_following.values()))\n    # Randomly sample the next word \n    weights = counts / np.sum(counts)\n    return np.random.choice(following_words, p = weights)\n\nNow to generate a description from scratch, we just use a loop to continuously generate the next word! The loop stops when we either hit the special end-os-sentence token, or when we reach a maximum description length.\n\ndef generate_description(prompt):  \n    \"\"\"Generate a wine descriptions given a prompt\"\"\"\n    prompt_doc = nlp(prompt)\n    \n    # set up the while loop\n    current_text = prompt\n    last_word = prompt_doc[-1].text\n    not_end_token = True\n    max_desc_length = 100\n    c = 0\n    \n    while not_end_token and c &lt; max_desc_length:\n        next_word = gen_next_word(last_word)\n        if next_word == end_token:\n            not_end_token = False\n        else:\n            current_text += \" \"+next_word\n            last_word = next_word\n            c += 1\n    \n    return current_text"
  },
  {
    "objectID": "posts/2020-10-15-wine-review-generation.html#trying-it-out",
    "href": "posts/2020-10-15-wine-review-generation.html#trying-it-out",
    "title": "The Markovian Sommelier",
    "section": "Trying it out!",
    "text": "Trying it out!\nNow we can generate our own wine reviews! Let’s look at a few examples:\n\ngenerate_description(\"A fruity merlot, with a smoky\")\n\n\"A fruity merlot, with a smoky oak . The black tea and toasty oak , apricot , allied to the next six years of lively , it 's an apéritif wine very tight and soft , it too extracted Malbec . Best now . Now–2014 .\"\n\n\n\ngenerate_description(\"A full bodied cabernet\")\n\n\"A full bodied cabernet sauvignon . It has honey , it 's a delicious , and berry fruits and rich future .\"\n\n\n\ngenerate_description(\"Spicy\")\n\n'Spicy cinnamon , it would pair with hearty mouthful of Pinot they are tougher , currants , cherries lead to the finish .'\n\n\n\ngenerate_description(\"This wine is terrible\")\n\n'This wine is terrible flaws here . In the black fruit . It feels tight tannins , luscious and fresh and sophisticated notes , this wine offers aromas emerge with ample cherry flavors . The finish is very impressive is a bit of cherry , which offers a shame to soften . In the ripe and Mourvèdre , with suggesting wet cement , juicy and bitter , this 100 % Syrah with just yearning to say that will put in French oak flavors are certified - dimensional in the perfumes , packed with mixed with mature fruit and minerality and a final indication of'"
  },
  {
    "objectID": "posts/2020-10-15-wine-review-generation.html#conclusion",
    "href": "posts/2020-10-15-wine-review-generation.html#conclusion",
    "title": "The Markovian Sommelier",
    "section": "Conclusion",
    "text": "Conclusion\nThere we have it! A (very rudimentary) text generation model!\nThe descriptions certainly aren’t great - I don’t think any human would be fooled! However, given how rudimentary our model is, the results are surprisingly good. The sentences are mostly coherent, and they also do well at capturing the vocabulary and phrases distinctive of the wine description genre! This shows how even the simplest model can “learn” features distinctive of the dataset it was trained on.\nOf course we could improve on this model by using 3-grams or 4-grams instead of bigrams, which would let us capture more context. Or, we could use NLP methods that are much better than Markov chains! Recurrent neural networks, transformers, etc… Maybe we’ll look at those in a future notebook.\nIn the meantime, enjoy this Markovian Sommelier!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob’s Blog",
    "section": "",
    "text": "This is my site for sporadically posting things, built with quarto. Welcome!\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nConfiguring Zotero and LaTeX\n\n\n\n\n\nReference management for LaTeX documents\n\n\n\n\n\n\nMay 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nThe Markovian Sommelier\n\n\n\n\n\nModeling wine reviews with a Markov chain of bigrams\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Camera Obscura\n\n\n\n\n\nDIY Photography\n\n\n\n\n\n\nSep 26, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my site for sporadically posting things, built with quarto. Welcome!"
  }
]